Frameworks and Concepts for Combining Vision Models and Large Language Models (LLMs) on Edge‑Based Robot Platforms
Introduction

Robots operating in unstructured environments must recognize objects, understand natural‑language instructions, and plan actions. Vision models provide perception by mapping camera data into semantically rich features, while LLMs interpret textual commands and perform high‑level reasoning. The challenge is to combine these models on resource‑constrained edge devices so that robots can perform tasks they were never explicitly trained for. This report reviews relevant research, including the Possibility Graph method for high‑level planning and modern vision‑language‑action (VLA) frameworks that unify perception and language understanding. It also summarizes methods for running such systems on edge platforms (e.g., Jetson or small PCs) through quantization, model partitioning and efficient architectures.
Possibility Graph for Multi‑Action Planning

Humanoid robots must combine different locomotion modes (walking, crawling, jumping) to traverse complex environments. Traditional planning explores each action type separately, resulting in slow, combinatorial search. Possibility Graphs provide a general planning framework that accelerates search by using quick tests that classify candidate motion segments as impossible, possible, or indeterminate. Key ideas:

Necessary/sufficient conditions: Each action type has necessary conditions (quick, low‑dimensional approximations of the constraint manifold) and sufficient conditions (conditions that guarantee the action is valid). The planner samples new vertices in a shared low‑dimensional “possibility exploration space” and checks whether they satisfy the necessary conditions, allowing rapid rejection of impossible actions without expensive inverse‑kinematics checks
ar5iv.labs.arxiv.org
. Sufficient conditions represent feasible action parameters and are used to connect segments.

Unified graph for multiple actions: Rather than maintaining separate graphs, the method builds a single possibility graph for all action types. Transitional vertices connect sub‑graphs corresponding to walking, crawling and jumping. The algorithm iteratively samples directions in the exploration space and interlaces expansion among actions, ensuring that transitions between action types are considered without re‑starting search
ar5iv.labs.arxiv.org
. This unified representation enables the planner to quickly generate sequences that include multiple behaviors.

By leveraging low‑dimensional projections and simple feasibility checks, the possibility graph method drastically reduces planning time and produces motion sequences that adapt to tasks not specifically trained (e.g., combining walking and crawling across unstructured terrain). It provides an example of abstraction and conditional reasoning that can inspire frameworks combining perception and LLMs.
Vision‑Language‑Action (VLA) Frameworks

Modern VLA models fuse visual observations, language instructions and robot actions in a single neural architecture. This unification allows the model to interpret natural‑language commands and generate control actions directly, avoiding brittle pipelines. Research has proposed several architectures:
Unified Embedding of Vision, Language and Action

VLA models represent sensor data (RGB‑D images or voxelized point clouds), natural‑language instructions and robot state as prefix tokens embedded into a shared latent space. Transformer architectures then process these sequences and output action tokens that can be decoded into joint commands. The tokenization allows the model to reason across modalities: by concatenating vision and language tokens, the model understands both “what” to do and “where”
arxiv.org
. Representing actions as discrete tokens enables the same model to output natural‑language responses or low‑level robot actions, facilitating co‑training on Internet‑scale vision‑language data and robot trajectories
robotics-transformer2.github.io
.
PaLM‑E and RT‑2: Large Models for Robotics

PaLM‑E couples Google’s PaLM language model with a ViT vision encoder. Images and robot states are encoded into the word‑embedding space and appended to the prompt. PaLM‑E generates text that can represent high‑level plans; a separate controller converts the output into low‑level actions. The model shows positive transfer: training on vision‑language tasks improves robot learning, enabling zero‑shot tasks such as retrieving a bag of chips or picking the green block
research.google
research.google
.

RT‑2 (Robot Transformer 2) improves this idea by co‑finetuning a vision‑language model on robotic trajectories and vision‑language tasks simultaneously. Actions are expressed as text tokens and included in the training corpus, unifying the representation of words and actions. RT‑2 demonstrates emergent reasoning abilities, such as choosing the smallest object or selecting a drink for a tired person, and generalizes to novel objects and instructions not present in the robot data
robotics-transformer2.github.io
.
Modular Frameworks

    LM‑Nav: A modular system for navigation that uses a pre‑trained visual navigation model (ViNG) for topological mapping, CLIP for grounding landmarks, and GPT‑3 for extracting landmarks from natural language. This combination enables long‑distance navigation without any language‑annotated robot data, demonstrating how off‑the‑shelf models can be orchestrated to execute tasks
    ar5iv.labs.arxiv.org
    .

    CLIPort: Combines the semantic richness of CLIP with the spatial precision of TransporterNets. A “what” stream identifies objects and a “where” stream provides pick‑and‑place positions. This architecture solves multi‑task tabletop manipulation from natural language using only ~179 demonstration pairs and generalizes to unseen objects and semantic concepts
    cliport.github.io
    . TransporterNets handle precise spatial reasoning, while CLIP supplies general semantic grounding.

    RoboGround: Introduces grounding masks—intermediate spatial representations generated by a vision‑language grounding model. Mask‑guided policies use these masks to focus on relevant regions, improving generalization to unseen objects and placements
    openaccess.thecvf.com
    openaccess.thecvf.com
    . This shows that explicit spatial guidance can bridge the gap between high‑level language understanding and fine motor control.

    PerAct / Perceiver‑Actor and Tiny Actions: PerAct uses a Perceiver Transformer to encode language goals and voxelized 3‑D observations; it outputs discretized pick‑and‑place actions. It outperforms image‑to‑action baselines on 18 RLBench tasks. Recent methods like TinyVLA replace autoregressive action token generation with diffusion-based policy heads and high‑speed multimodal encoders. TinyVLA achieves faster inference and better data efficiency, outperforming large models while using <1 B parameters
    ar5iv.labs.arxiv.org
    ar5iv.labs.arxiv.org
    .

Edge‑Oriented VLA Models

Large VLA models (PaLM‑E, RT‑2) require considerable computational resources and are typically cloud-based. Edge devices such as Nvidia Jetson or small PCs demand lightweight architectures and efficient inference techniques:

    Edge VLA (EdgeVLA) — Designed for real‑time inference (30–50 Hz) on low‑power devices. It eliminates autoregressive decoding by predicting the 6‑DoF end‑effector pose directly and uses compact encoders: SigLIP and DINOv2 for vision and Qwen2‑0.5B for language
    arxiv.org
    . This yields a 7× speed‑up while retaining comparable performance to larger models
    arxiv.org
    . The design acknowledges the difficulty of running large models on Jetson‑Nano‑class hardware
    arxiv.org
    .

    Tiny VLA — Further reduces model size and inference time by leveraging FastViT vision encoders and a tiny (128‑dimensional) language encoder. A diffusion‑based policy head outputs continuous actions directly, eliminating discrete action tokens and speeding up inference. TinyVLA outperforms OpenVLA in both speed and task success with only ~50 M parameters
    ar5iv.labs.arxiv.org
    .

    Quantization and Edge Tools — Quantization converts floating‑point weights to lower‑precision (INT8) representations, reducing memory and doubling throughput while retaining ~97 % of full‑precision accuracy
    arxiv.org
    . Frameworks like llama_ros integrate quantized LLaMA models into ROS2, allowing offline natural‑language planning on robots; quantized models drastically reduce memory and compute demands, making them feasible on small edge devices
    arxiv.org
    arxiv.org
    . Hardware‑aware optimizations (kernel fusion, graph rewrites) and compressed action tokenization further accelerate inference
    arxiv.org
    .

    Edge‑Compute Digital Twin — A proof‑of‑concept system uses an Intel RealSense camera, an offline speech recognizer (VOSK), and a local LLM (e.g., Mistral 7‑B via Ollama) to enable natural interactions with an assistive robot arm. Running on a local PC eliminates cloud latency and enables real‑time perception and decision‑making
    ar5iv.labs.arxiv.org
    ar5iv.labs.arxiv.org
    . The LLM interprets user commands and directs the vision system to select the correct object by color or preference, demonstrating integration of language and vision at the edge【319842689744015†L245-L286】.

    Digital Twin with Local LLMs — In a healthcare‑task scenario, a robot uses a depth camera and an in‑situ LLM (Mistral 7‑B) to maintain a digital twin in simulation. The physical and virtual environments are kept synchronized with 98.11 % accuracy. The system processes user voice commands locally and updates the digital twin in real time, enabling untrained users to interact with the robot and perform tasks like sorting microscope slides
    mdpi.com
    mdpi.com
    .

Enabling Robots to Perform Novel Tasks

The reviewed frameworks share several concepts that allow robots to perform tasks they were never explicitly trained for:

    Cross‑modal pre‑training and co‑finetuning: Models like RT‑2 and PaLM‑E leverage large vision‑language corpora. By fine‑tuning on small amounts of robot data, they transfer general knowledge (e.g., object categories, commonsense reasoning) to robotics tasks, enabling zero‑shot or few‑shot generalization
    robotics-transformer2.github.io
    research.google
    .

    Unified token representation: Treating actions as text tokens unifies action prediction with language generation. The model can respond to instructions or output commands using the same underlying representation, enabling cross‑domain learning and reasoning
    robotics-transformer2.github.io
    .

    Spatial grounding and intermediate representations: Approaches like CLIPort and RoboGround explicitly ground language into spatial representations. Two‑stream architectures (semantic vs. spatial) and mask guidance help the model localize objects and plan precise actions, improving generalization to new object types
    cliport.github.io
    openaccess.thecvf.com
    .

    Modular combination of pre‑trained models: LM‑Nav shows that off‑the‑shelf visual navigation, vision‑language and language models can be orchestrated without training to perform complex tasks such as long‑distance navigation
    ar5iv.labs.arxiv.org
    . This modularity allows rapid adaptation to new tasks by swapping modules.

    Efficient inference for edge deployment: To operate on resource‑constrained robots, models must use small encoders, quantization, diffusion policies or parallel decoding, as demonstrated by EdgeVLA, TinyVLA and llama_ros
    arxiv.org
    ar5iv.labs.arxiv.org
    arxiv.org
    . These techniques allow real‑time control while maintaining generalization capabilities.

Comparison of Edge‑Compatible VLA Models

The table below summarizes key properties of some edge‑compatible VLA models. The goal is to highlight differences without long sentences.
Model	Vision Encoder / Language Model	Action Output	Parameters (approx.)	Key Features
Edge VLA
arxiv.org
arxiv.org
	SigLIP + DINOv2 / Qwen2‑0.5B	Direct 6‑DoF pose (non‑autoregressive)	~0.5 B	Real‑time (30–50 Hz); runs on Jetson Nano; comparable accuracy to large VLA models
Tiny VLA
ar5iv.labs.arxiv.org
	FastViT / 128‑dim language encoder	Diffusion policy head (continuous actions)	~50 M	5× faster than OpenVLA; no large robot datasets needed; strong generalization
llama_ros
arxiv.org
arxiv.org
	Quantized LLaMA (via llama.cpp)	Text plan → external controller	Dependent on LLaMA size	ROS2 integration of quantized LLM; offline planning on edge hardware
Assistive arm (Edge)
ar5iv.labs.arxiv.org
	RealSense camera + VOSK speech recognition; GPT‑3.5/4 via API or local	Task‑specific actions via ROS	-	Demonstrates voice‑controlled edge robotics using local perception and LLM
Digital Twin with LLM
mdpi.com
	RealSense depth camera + local LLM (Mistral 7‑B)	Synchronizes physical and simulated robot	-	Achieves 98.11 % synchronization and natural language control
Conclusion and Recommendations

Combining vision models and LLMs on edge‑based robotics platforms is rapidly evolving. Key insights include:

    Abstraction and planning: The Possibility Graph illustrates how high‑level reasoning can be structured around necessary/sufficient conditions, guiding exploration and enabling untrained behaviors. This paradigm can inspire high‑level modules that quickly reject impossible actions before delegating to expensive reasoning.

    Unified multimodal models: VLA frameworks unify perception, language and action into a single representation, enabling cross‑modal reasoning and transfer learning. Representing actions as tokens or continuous vectors allows the model to learn from both internet data and robot trajectories and to generalize to unseen tasks.

    Edge‑friendly designs: Real‑time robotic control requires small encoders, non‑autoregressive decoders, quantization and hardware‑aware optimizations. EdgeVLA and TinyVLA demonstrate that careful model architecture choices can achieve high performance on low‑power devices. Tools like llama_ros make it possible to run LLMs offline in ROS systems, and digital twin frameworks show how local LLMs can coordinate with real robots.

    Future work: Continued research is needed into compressing multimodal models, designing hardware‑accelerated inference engines and developing open datasets of robot interactions. Combining the rapid planning principles of the possibility graph with VLA models may offer new ways to quickly determine feasible sequences of actions in complex tasks.

By integrating these methods—efficient perception, language understanding, spatial grounding and edge‑optimized architectures—robotic platforms can achieve robust, adaptable behavior and handle tasks beyond their original training.